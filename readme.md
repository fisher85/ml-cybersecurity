# Machine Learning in Cybersecurity

## Demos

It's recommended to open [Jupyter](http://jupyter.org) notebooks in [Google Colab–æratory](https://colab.research.google.com).

### Acquaintance with the Stanford 2019 AI Index Report

https://colab.research.google.com/github/fisher85/ml-cybersecurity/blob/master/ai-index-report-2019-research/cybersecurity_startups.ipynb

### Fraud detection

https://colab.research.google.com/github/fisher85/ml-cybersecurity/blob/master/python-logistic-regression-fraud-detection/python-logistic-regression-fraud-detection.ipynb

### WAF (Web Application Firewall) Poisoning

https://colab.research.google.com/github/fisher85/ml-cybersecurity/blob/master/python-waf/machine-learning-waf.ipynb

### Intrusion Detection System

Classifier comparison: https://colab.research.google.com/github/fisher85/ml-cybersecurity/blob/master/python-web-attack-detection/classifier-comparison.ipynb

Web attack detection: https://colab.research.google.com/github/fisher85/ml-cybersecurity/blob/master/python-web-attack-detection/web-attack-detection.ipynb

Web attack detection using CNN-BiLSTM: https://colab.research.google.com/github/fisher85/ml-cybersecurity/blob/master/python-web-attack-detection/web-attack-detection-using-CNN-BiLSTM.ipynb

### Adversarial Attacks

Defending ML IDS against an evasion attack using adversarial training: https://colab.research.google.com/github/fisher85/ml-cybersecurity/blob/master/adversarial-attacks/evasion-attack.ipynb

Comparison of adversarial attacks: https://colab.research.google.com/github/fisher85/ml-cybersecurity/blob/master/adversarial-attacks/comparison_of_adversarial_attacks.ipynb

Iterative adversarial training with the HSJA attack and a Random Forest model using CICIDS2017 dataset: https://colab.research.google.com/github/fisher85/ml-cybersecurity/blob/master/adversarial-attacks/iterative_adversarial_training_with_HSJA.ipynb

Adversarial Example on MNIST (Manual Perturbation):
https://colab.research.google.com/github/fisher85/ml-cybersecurity/blob/master/adversarial-attacks/MNIST_adversarial_example_manual_perturbation.ipynb

FGSM Step-by-Step Attack on a Simple Neural Network:
https://colab.research.google.com/github/fisher85/ml-cybersecurity/blob/master/adversarial-attacks/FGSM_step_by_step.ipynb
